"""
æ€§èƒ½ç›‘æ§å’Œç¼“å­˜å·¥å…·
âš ï¸ é—ç•™ä»£ç ï¼šä¸»è¦ç”¨äºæ—§ç‰ˆAIæœåŠ¡å’Œæ€§èƒ½ç›‘æ§API

å½“å‰ä½¿ç”¨æ­¤æ–‡ä»¶çš„æœåŠ¡ï¼š
- /api/v1/performance - æ€§èƒ½ç›‘æ§API âœ…
- /api/v1/chat - æ—§ç‰ˆAIå¯¹è¯ï¼ˆå·²è¢«Agentæ›¿ä»£ï¼‰

æ–°ç‰ˆAgentæœåŠ¡ï¼ˆagent_service.pyï¼‰å·²å®ç°æ›´å®Œå–„çš„é‡è¯•æœºåˆ¶ï¼Œä¸å†ä½¿ç”¨æ­¤æ–‡ä»¶çš„é‡è¯•åŠŸèƒ½ã€‚
"""
import time
from typing import Dict, Any, Optional, List

from app.core.config import settings


class AICache:
    """ç®€å•çš„å†…å­˜ç¼“å­˜å®ç°"""
    
    def __init__(self):
        self._cache: Dict[str, Dict[str, Any]] = {}
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if not settings.AI_ENABLE_CACHE:
            return None
        
        if key in self._cache:
            item = self._cache[key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - item['timestamp'] < settings.AI_CACHE_TTL:
                print(f"[ç¼“å­˜å‘½ä¸­] {key[:50]}...")
                return item['value']
            else:
                # è¿‡æœŸï¼Œåˆ é™¤
                del self._cache[key]
        
        return None
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        if not settings.AI_ENABLE_CACHE:
            return
        
        self._cache[key] = {
            'value': value,
            'timestamp': time.time()
        }
        
        # ç®€å•çš„LRUï¼šç¼“å­˜è¶…è¿‡1000é¡¹æ—¶æ¸…ç†æ—§çš„
        if len(self._cache) > 1000:
            # åˆ é™¤æœ€æ—§çš„100é¡¹
            sorted_items = sorted(
                self._cache.items(),
                key=lambda x: x[1]['timestamp']
            )
            for key, _ in sorted_items[:100]:
                del self._cache[key]
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()


# å…¨å±€ç¼“å­˜å®ä¾‹
_ai_cache = AICache()


def get_cache() -> AICache:
    """è·å–ç¼“å­˜å®ä¾‹"""
    return _ai_cache


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: List[Dict[str, Any]] = []
    
    def record(self, operation: str, duration: float, success: bool, **kwargs):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        self.metrics.append({
            'operation': operation,
            'duration': duration,
            'success': success,
            'timestamp': time.time(),
            **kwargs
        })
        
        # åªä¿ç•™æœ€è¿‘100æ¡
        if len(self.metrics) > 100:
            self.metrics = self.metrics[-100:]
        
        # æ‰“å°æ€§èƒ½æ—¥å¿—
        status = "âœ“" if success else "âœ—"
        print(f"[æ€§èƒ½] {status} {operation}: {duration:.2f}ç§’")
    
    def get_stats(self, operation: Optional[str] = None) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        if operation:
            filtered = [m for m in self.metrics if m['operation'] == operation]
        else:
            filtered = self.metrics
        
        if not filtered:
            return {}
        
        durations = [m['duration'] for m in filtered]
        success_count = sum(1 for m in filtered if m['success'])
        
        return {
            'total_calls': len(filtered),
            'success_rate': success_count / len(filtered) * 100,
            'avg_duration': sum(durations) / len(durations),
            'min_duration': min(durations),
            'max_duration': max(durations)
        }


# å…¨å±€æ€§èƒ½ç›‘æ§å™¨
_performance_monitor = PerformanceMonitor()


def get_monitor() -> PerformanceMonitor:
    """è·å–æ€§èƒ½ç›‘æ§å™¨"""
    return _performance_monitor


# ==================== ä»¥ä¸‹ä¸ºé—ç•™ä»£ç  ====================
# âš ï¸ æ³¨æ„ï¼š
# - retry_with_backoff: å·²è¢« agent_service.py ä¸­çš„ _retry_tool_call æ›¿ä»£
# - OptimizedAIBase: ä»…ç”¨äºæ—§ç‰ˆ ai_service.pyï¼Œå»ºè®®è¿ç§»åˆ°Agent
# ======================================================

# ä¿ç•™æ­¤å‡½æ•°ä»…ç”¨äºå‘åå…¼å®¹
async def retry_with_backoff(
    func,
    max_retries: int = None,
    initial_delay: float = None,
    backoff_factor: float = 2.0
):
    """
    å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•è£…é¥°å™¨ï¼ˆé—ç•™å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨agent_service.pyä¸­çš„å®ç°ï¼‰
    
    Args:
        func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹å»¶è¿Ÿ
        backoff_factor: é€€é¿å› å­
    """
    import asyncio
    max_retries = max_retries or settings.AI_MAX_RETRIES
    initial_delay = initial_delay or settings.AI_RETRY_DELAY
    
    last_exception = None
    
    for attempt in range(max_retries + 1):
        try:
            return await func()
        except Exception as e:
            last_exception = e
            
            if attempt < max_retries:
                delay = initial_delay * (backoff_factor ** attempt)
                if settings.DEBUG:
                    print(f"[é‡è¯•-æ—§ç‰ˆ] ç¬¬{attempt + 1}æ¬¡å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯•: {str(e)[:100]}")
                await asyncio.sleep(delay)
            else:
                if settings.DEBUG:
                    print(f"[é‡è¯•-æ—§ç‰ˆ] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})ï¼Œæ”¾å¼ƒ")
    
    raise last_exception


class OptimizedAIBase:
    """
    ä¼˜åŒ–çš„AIæœåŠ¡åŸºç±»ï¼ˆé—ç•™ç±»ï¼Œä»…ç”¨äºæ—§ç‰ˆai_service.pyï¼‰
    
    âš ï¸ æ–°é¡¹ç›®è¯·ä½¿ç”¨ agent_service.py ä¸­çš„ TravelPlannerAgent
    """
    
    def __init__(self):
        self.cache = get_cache()
        self.monitor = get_monitor()
    
    def create_llm(
        self,
        temperature: float = None,
        max_tokens: int = None,
        streaming: bool = False,
        **kwargs
    ):
        """
        åˆ›å»ºLLMå®ä¾‹ï¼ˆé—ç•™æ–¹æ³•ï¼‰
        
        Args:
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
        """
        from langchain_openai import ChatOpenAI
        
        temperature = temperature if temperature is not None else settings.AI_TEMPERATURE_BALANCED
        max_tokens = max_tokens if max_tokens is not None else settings.AI_MAX_TOKENS
        timeout = settings.AI_STREAM_TIMEOUT if streaming else settings.AI_TIMEOUT
        
        return ChatOpenAI(
            model=settings.DEEPSEEK_MODEL,
            openai_api_key=settings.DEEPSEEK_API_KEY,
            openai_api_base=settings.DEEPSEEK_API_BASE,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            streaming=streaming,
            **kwargs
        )
    
    def generate_cache_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json
        # å°†å‚æ•°åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²
        key_data = {
            'args': args,
            'kwargs': kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True, ensure_ascii=False)
        
        # ç”ŸæˆMD5å“ˆå¸Œ
        return hashlib.md5(key_str.encode()).hexdigest()
    
    async def call_with_cache_and_retry(
        self,
        operation: str,
        func,
        cache_key: Optional[str] = None,
        enable_cache: bool = True
    ):
        """
        å¸¦ç¼“å­˜å’Œé‡è¯•çš„AIè°ƒç”¨ï¼ˆé—ç•™æ–¹æ³•ï¼‰
        
        Args:
            operation: æ“ä½œåç§°ï¼ˆç”¨äºç›‘æ§ï¼‰
            func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
            cache_key: ç¼“å­˜é”®
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        start_time = time.time()
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if enable_cache and cache_key:
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                duration = time.time() - start_time
                self.monitor.record(
                    operation=operation,
                    duration=duration,
                    success=True,
                    cached=True
                )
                return cached_result
        
        # æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦é‡è¯•ï¼‰
        try:
            result = await retry_with_backoff(func)
            
            # ç¼“å­˜ç»“æœ
            if enable_cache and cache_key:
                self.cache.set(cache_key, result)
            
            # è®°å½•æ€§èƒ½
            duration = time.time() - start_time
            self.monitor.record(
                operation=operation,
                duration=duration,
                success=True,
                cached=False
            )
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            self.monitor.record(
                operation=operation,
                duration=duration,
                success=False,
                error=str(e)[:100]
            )
            raise
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self.monitor.get_stats()


# ==================== æ•´åˆè¯´æ˜ ====================
# 
# æ­¤æ–‡ä»¶å·²æ•´åˆä¼˜åŒ–ï¼ˆ2025-10-10ï¼‰ï¼š
#
# âœ… ä¿ç•™éƒ¨åˆ†ï¼š
# - AICacheï¼šç¼“å­˜å·¥å…·ï¼ˆæ€§èƒ½ç›‘æ§APIä½¿ç”¨ï¼‰
# - PerformanceMonitorï¼šæ€§èƒ½ç›‘æ§ï¼ˆ/api/v1/performanceä½¿ç”¨ï¼‰
# 
# âš ï¸ é—ç•™éƒ¨åˆ†ï¼ˆå‘åå…¼å®¹ï¼‰ï¼š
# - retry_with_backoffï¼šæ—§ç‰ˆé‡è¯•å‡½æ•°
# - OptimizedAIBaseï¼šæ—§ç‰ˆåŸºç±»ï¼ˆä»…ai_service.pyä½¿ç”¨ï¼‰
#
# ğŸ¯ æ¨èè¿ç§»è·¯å¾„ï¼š
# - æ–°åŠŸèƒ½ â†’ agent_service.pyï¼ˆTravelPlannerAgentï¼‰
# - æ—§AIå¯¹è¯ â†’ è€ƒè™‘è¿ç§»åˆ°Agentæˆ–ä¿æŒç°çŠ¶
# - æ€§èƒ½ç›‘æ§ â†’ ç»§ç»­ä½¿ç”¨æ­¤æ–‡ä»¶çš„monitorå’Œcache
#
# ==================================================
